# GAIA Benchmark - Model Comparison

**Dataset:** `sample`

This table compares results across all tested models for this dataset. Results are sorted by overall accuracy.

## Best per model

| Provider | Model | Overall | Level 1 | Level 2 | Level 3 | Questions | Errors | Tokens | Latency (s) |
|----------|-------|---------|---------|---------|---------|-----------|--------|--------|-------------|
| openai | gpt-5-mini | 0.0% | 0.0% | 0.0% | 0.0% | 3 | 0 | 0 | 7.8 |


## Latest run per model

| Provider | Model | Overall | Questions | Errors | Tokens | Latency (s) | Timestamp |
|----------|-------|---------|-----------|--------|--------|-------------|-----------|
| openai | gpt-5-mini | 0.0% | 3 | 0 | 0 | 7.8 | 2026-02-06T10:25:29.184361 |

## Notes

- This comparison is for a **non-official dataset source** (e.g. sample/jsonl).
- Official GAIA leaderboard scores are **not comparable** unless `--dataset gaia` is used.


---
*Updated automatically by ElizaOS GAIA Benchmark Runner*
