# GAIA Benchmark - Model Comparison

**Dataset:** `sample`

This table compares results across all tested models for this dataset. Results are sorted by overall accuracy.

## Best per model

| Provider | Model | Overall | Level 1 | Level 2 | Level 3 | Questions | Errors | Tokens | Latency (s) |
|----------|-------|---------|---------|---------|---------|-----------|--------|--------|-------------|
| groq | qwen/qwen3-32b | 100.0% | 100.0% | 100.0% | 100.0% | 5 | 0 | 53,158 | 3.2 |


## Latest run per model

| Provider | Model | Overall | Questions | Errors | Tokens | Latency (s) | Timestamp |
|----------|-------|---------|-----------|--------|--------|-------------|-----------|
| groq | qwen/qwen3-32b | 100.0% | 5 | 0 | 53,158 | 3.2 | 2026-02-11T23:13:08.243106 |

## Notes

- This comparison is for a **non-official dataset source** (e.g. sample/jsonl).
- Official GAIA leaderboard scores are **not comparable** unless `--dataset gaia` is used.


---
*Updated automatically by ElizaOS GAIA Benchmark Runner*
