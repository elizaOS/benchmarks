# GAIA Benchmark - Model Comparison

**Dataset:** `sample`

This table compares results across all tested models for this dataset. Results are sorted by overall accuracy.

## Best per model

| Provider | Model | Overall | Level 1 | Level 2 | Level 3 | Questions | Errors | Tokens | Latency (s) |
|----------|-------|---------|---------|---------|---------|-----------|--------|--------|-------------|
| groq | qwen/qwen3-32b | 66.7% | 100.0% | 0.0% | 0.0% | 3 | 0 | 183,847 | 18.3 |


## Latest run per model

| Provider | Model | Overall | Questions | Errors | Tokens | Latency (s) | Timestamp |
|----------|-------|---------|-----------|--------|--------|-------------|-----------|
| groq | qwen/qwen3-32b | 66.7% | 3 | 0 | 183,847 | 18.3 | 2026-02-12T04:20:48.655281 |

## Notes

- This comparison is for a **non-official dataset source** (e.g. sample/jsonl).
- Official GAIA leaderboard scores are **not comparable** unless `--dataset gaia` is used.


---
*Updated automatically by ElizaOS GAIA Benchmark Runner*
