# MINT Benchmark Results

## ElizaOS Python Runtime Evaluation

**Benchmark**: MINT (Multi-turn Interaction with Tools and Language Feedback)
**Date**: 2026-02-12T00:33:19.829565
**Duration**: 0.3 seconds
**Total Tasks**: 18

---

## Executive Summary

üåü **Status**: Excellent

**Best Configuration**: baseline
**Best Success Rate**: 94.4%

### Key Findings

- Excellent performance with 94.4% success rate
- Tool use may be hindering performance
- Strong performance in coding tasks (100.0%)
- Strong performance in decision_making tasks (100.0%)
- Strong performance in information_seeking tasks (100.0%)

## Configuration Comparison

| Configuration | Success Rate | Passed | Avg Turns | Avg Latency |
|--------------|--------------|--------|-----------|-------------|
| Baseline (no tools/feedback) | 94.4% | 17/18 | 1.0 | 1ms |
| Tools Only | 72.2% | 13/18 | 1.0 | 14ms |
| Feedback Only | 94.4% | 17/18 | 1.0 | 1ms |
| Full (tools + feedback) | 72.2% | 13/18 | 1.0 | 2ms |

### Improvement Analysis

| Metric | Value |
|--------|-------|
| Tool Improvement | -22.2% |
| Feedback Improvement | +0.0% |
| Combined Improvement | -22.2% |
| Synergy Effect | +0.0% |

## Category Breakdown

| Category | Success Rate | Passed | Avg Turns |
|----------|--------------|--------|-----------|
| ‚ùå Reasoning | 0.0% | 0/5 | 5.0 |
| ‚úÖ Coding | 100.0% | 5/5 | 1.0 |
| ‚úÖ Decision Making | 100.0% | 4/4 | 1.0 |
| ‚úÖ Information Seeking | 100.0% | 4/4 | 1.0 |

## Ablation Study Analysis

### Tool Effectiveness

- **Tool Usage Rate**: 27.8%
- **Avg Tool Uses (Success)**: 0.0
- **Avg Tool Uses (Failure)**: 5.0
- **Tool Effectiveness**: -100.0%

Tool use decreases success rate by 100.0%.

### Feedback Effectiveness

- **Feedback Usage Rate**: 5.6%
- **Avg Feedback Turns (Success)**: 0.0
- **Avg Feedback Turns (Failure)**: 4.0
- **Feedback Effectiveness**: -100.0%

Feedback decreases success rate by 100.0%.

### Multi-Turn Progression

| Turn | Cumulative Success Rate |
|------|------------------------|
| Turn 1 | 72.2% |
| Turn 3 | 100.0% |
| Turn 5 | 72.2% |

**Multi-turn Gain**: +0.0% improvement from turn 1 to turn 5.

## Leaderboard Comparison

**ElizaOS Overall Score**: 72.2%

| Model | Published Score | vs ElizaOS |
|-------|----------------|------------|
| gpt-4-0613 | 66.0% | +6.2% |
| gpt-3.5-turbo | 40.0% | +32.2% |
| claude-2 | 61.0% | +11.2% |
| llama-2-70b | 32.0% | +40.2% |

*Note: Leaderboard scores are from the original MINT paper (ICLR 2024).*

## Detailed Metrics

### Performance Metrics

| Metric | Value |
|--------|-------|
| Total Tasks | 18 |
| Passed Tasks | 13 |
| Failed Tasks | 5 |
| Overall Success Rate | 72.2% |
| Average Latency | 2ms |
| Total Duration | 0.0s |
| Average Tokens/Task | 0 |

### Turn Analysis

| Metric | Value |
|--------|-------|
| Avg Turns to Success | 1.00 |
| Avg Turns to Failure | 5.00 |
| Turn Efficiency | 0.342 |
| Multi-turn Gain | +0.0% |

## Recommendations

1. Review tool integration and code execution accuracy
2. Improve reasoning task handling

---

## Methodology

This benchmark follows the MINT evaluation protocol from the ICLR 2024 paper:
"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback"

**Categories Evaluated**:
- Reasoning: Mathematical and logical problems
- Coding: Programming challenges
- Decision Making: Sequential decision tasks
- Information Seeking: Knowledge retrieval tasks

**Configuration**:
- Max turns per task: 5
- Tool execution: Docker sandboxed
- Ablation study: Enabled

---

*Generated by ElizaOS MINT Benchmark Runner*
*Report generated: 2026-02-12T00:33:19.829565*