# SWE-bench Benchmark Results

## Summary

| Metric | Value |
|--------|-------|
| **Variant** | lite |
| **Mode** | agent |
| **Total Instances** | 1 |
| **Resolved** | 0 |
| **Resolve Rate** | 0.0% |
| **Apply Rate** | 0.0% |
| **Avg Duration** | 38.6s |
| **Avg Tokens** | 470 |

## Leaderboard Comparison

| System | Score |
|--------|-------|
| **ElizaOS (This Run)** | **0.0%** |
| OpenHands + Claude 3.5 Sonnet | 53.0% |
| Agentless + GPT-4o | 33.2% |
| SWE-agent + GPT-4 | 33.2% |
| AutoCodeRover + GPT-4o | 30.7% |
| Aider + Claude 3.5 Sonnet | 26.3% |
| Aider + GPT-4o | 18.3% |
| RAG + GPT-4 | 6.7% |
| GPT-4 (no agent) | 1.7% |

**Estimated Rank**: #9 out of 9

## By Repository
| Repository | Total | Resolved | Rate |
|------------|-------|----------|------|
| astropy/astropy | 1 | 0 | 0.0% |

## Error Analysis

| Error Type | Count |
|------------|-------|
| No patch generated | 1 |

## Configuration

- Model: qwen/qwen3-32b
- Max Steps: 30
- Docker Evaluation: False
- Timeout: 600s

---
*Generated by ElizaOS SWE-bench Benchmark*
*Timestamp: 2026-02-14T20:02:15.095774*
