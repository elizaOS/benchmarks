# MINT Benchmark Results

## ElizaOS Python Runtime Evaluation

**Benchmark**: MINT (Multi-turn Interaction with Tools and Language Feedback)
**Date**: 2026-02-14T19:57:42.379542
**Duration**: 0.3 seconds
**Total Tasks**: 4

---

## Executive Summary

❌ **Status**: Needs Improvement

**Best Configuration**: baseline
**Best Success Rate**: 0.0%

### Key Findings

- Performance needs improvement (0.0% success rate)

## Configuration Comparison

| Configuration | Success Rate | Passed | Avg Turns | Avg Latency |
|--------------|--------------|--------|-----------|-------------|
| Baseline (no tools/feedback) | 0.0% | 0/4 | 0.0 | 18ms |
| Tools Only | 0.0% | 0/4 | 0.0 | 17ms |
| Feedback Only | 0.0% | 0/4 | 0.0 | 18ms |
| Full (tools + feedback) | 0.0% | 0/4 | 0.0 | 17ms |

### Improvement Analysis

| Metric | Value |
|--------|-------|
| Tool Improvement | +0.0% |
| Feedback Improvement | +0.0% |
| Combined Improvement | +0.0% |
| Synergy Effect | +0.0% |

## Category Breakdown

| Category | Success Rate | Passed | Avg Turns |
|----------|--------------|--------|-----------|
| ❌ Reasoning | 0.0% | 0/1 | 5.0 |
| ❌ Coding | 0.0% | 0/1 | 5.0 |
| ❌ Decision Making | 0.0% | 0/1 | 5.0 |
| ❌ Information Seeking | 0.0% | 0/1 | 5.0 |

## Ablation Study Analysis

### Tool Effectiveness

- **Tool Usage Rate**: 0.0%
- **Avg Tool Uses (Success)**: 0.0
- **Avg Tool Uses (Failure)**: 0.0
- **Tool Effectiveness**: +0.0%

Tool use decreases success rate by 0.0%.

### Feedback Effectiveness

- **Feedback Usage Rate**: 100.0%
- **Avg Feedback Turns (Success)**: 0.0
- **Avg Feedback Turns (Failure)**: 4.0
- **Feedback Effectiveness**: +0.0%

Feedback decreases success rate by 0.0%.

### Multi-Turn Progression

| Turn | Cumulative Success Rate |
|------|------------------------|
| Turn 1 | 0.0% |
| Turn 3 | 0.0% |
| Turn 5 | 0.0% |

**Multi-turn Gain**: +0.0% improvement from turn 1 to turn 5.

## Leaderboard Comparison

**ElizaOS Overall Score**: 0.0%

| Model | Published Score | vs ElizaOS |
|-------|----------------|------------|
| gpt-4-0613 | 66.0% | -66.0% |
| gpt-3.5-turbo | 40.0% | -40.0% |
| claude-2 | 61.0% | -61.0% |
| llama-2-70b | 32.0% | -32.0% |

*Note: Leaderboard scores are from the original MINT paper (ICLR 2024).*

## Detailed Metrics

### Performance Metrics

| Metric | Value |
|--------|-------|
| Total Tasks | 4 |
| Passed Tasks | 0 |
| Failed Tasks | 4 |
| Overall Success Rate | 0.0% |
| Average Latency | 17ms |
| Total Duration | 0.1s |
| Average Tokens/Task | 0 |

### Turn Analysis

| Metric | Value |
|--------|-------|
| Avg Turns to Success | 0.00 |
| Avg Turns to Failure | 5.00 |
| Turn Efficiency | 0.000 |
| Multi-turn Gain | +0.0% |

## Recommendations

1. Improve reasoning task handling
2. Improve coding task handling
3. Improve decision_making task handling
4. Improve information_seeking task handling

---

## Methodology

This benchmark follows the MINT evaluation protocol from the ICLR 2024 paper:
"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback"

**Categories Evaluated**:
- Reasoning: Mathematical and logical problems
- Coding: Programming challenges
- Decision Making: Sequential decision tasks
- Information Seeking: Knowledge retrieval tasks

**Configuration**:
- Max turns per task: 5
- Tool execution: Docker sandboxed
- Ablation study: Enabled

---

*Generated by ElizaOS MINT Benchmark Runner*
*Report generated: 2026-02-14T19:57:42.379542*